{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date |  Topic  |\n",
    "|------|------|\n",
    "|   21st  | LR & Perceptron|\n",
    "|   22and  | Decision tree & Naive|\n",
    "|   23rd  | SVM and ensemble|\n",
    "|   24th  | Model eval & train Tune|\n",
    "|  25th to 27th  | Project |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Gradient Descent`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "# Setting a random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# TODO: Fill in code in the function below to implement a gradient descent\n",
    "# step for linear regression, following a squared error rule. See the docstring\n",
    "# for parameters and returned variables.\n",
    "def MSEStep(X, y, W, b, learn_rate = 0.005):\n",
    "    \"\"\"\n",
    "    This function implements the gradient descent step for squared error as a\n",
    "    performance metric.\n",
    "    \n",
    "    Parameters\n",
    "    X : array of predictor features\n",
    "    y : array of outcome values\n",
    "    W : predictor feature coefficients\n",
    "    b : regression function intercept\n",
    "    learn_rate : learning rate\n",
    "\n",
    "    Returns\n",
    "    W_new : predictor feature coefficients following gradient descent step\n",
    "    b_new : intercept following gradient descent step\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill in code\n",
    "    pred=np.matmul(X,W)+b\n",
    "    err=y-pred\n",
    "    \n",
    "    W_new=W+learn_rate*np.matmul(err, X)\n",
    "    b_new=b+learn_rate*err.sum()\n",
    "    \n",
    "    \n",
    "    return W_new, b_new\n",
    "\n",
    "\n",
    "# The parts of the script below will be run when you press the \"Test Run\"\n",
    "# button. The gradient descent step will be performed multiple times on\n",
    "# the provided dataset, and the returned list of regression coefficients\n",
    "# will be plotted.\n",
    "def miniBatchGD(X, y, batch_size = 20, learn_rate = 0.005, num_iter = 25):\n",
    "    \"\"\"\n",
    "    This function performs mini-batch gradient descent on a given dataset.\n",
    "\n",
    "    Parameters\n",
    "    X : array of predictor features\n",
    "    y : array of outcome values\n",
    "    batch_size : how many data points will be sampled for each iteration\n",
    "    learn_rate : learning rate\n",
    "    num_iter : number of batches used\n",
    "\n",
    "    Returns\n",
    "    regression_coef : array of slopes and intercepts generated by gradient\n",
    "      descent procedure\n",
    "    \"\"\"\n",
    "    n_points = X.shape[0]\n",
    "    W = np.zeros(X.shape[1]) # coefficients\n",
    "    b = 0 # intercept\n",
    "    \n",
    "    # run iterations\n",
    "    regression_coef = [np.hstack((W,b))]\n",
    "    for _ in range(num_iter):\n",
    "        batch = np.random.choice(range(n_points), batch_size)\n",
    "        X_batch = X[batch,:]\n",
    "        y_batch = y[batch]\n",
    "        W, b = MSEStep(X_batch, y_batch, W, b, learn_rate)\n",
    "        regression_coef.append(np.hstack((W,b)))\n",
    "    \n",
    "    return regression_coef\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # perform gradient descent\n",
    "    data = np.loadtxt('data.csv', delimiter = ',')\n",
    "    X = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "    regression_coef = miniBatchGD(X, y)\n",
    "    \n",
    "    # plot the results\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure()\n",
    "    X_min = X.min()\n",
    "    X_max = X.max()\n",
    "    counter = len(regression_coef)\n",
    "    for W, b in regression_coef:\n",
    "        counter -= 1\n",
    "        color = [1 - 0.92 ** counter for _ in range(3)]\n",
    "        plt.plot([X_min, X_max],[X_min * W + b, X_max * W + b], color = color)\n",
    "    plt.scatter(X, y, zorder = 3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.07931]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([21.07931]).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add import statements\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Assign the data to predictor and outcome variables\n",
    "# TODO: Load the data\n",
    "train_data = pd.read_csv(\"data.csv\")\n",
    "X = train_data.iloc[:,:-1]\n",
    "y = train_data.iloc[:,-1]\n",
    "\n",
    "# TODO: Create the linear regression model with lasso regularization.\n",
    "lasso_reg = Lasso()\n",
    "\n",
    "# TODO: Fit the model.\n",
    "res=lasso_reg.fit(X,y)\n",
    "\n",
    "# TODO: Retrieve and print out the coefficients from the regression model.\n",
    "reg_coef = res.coef_\n",
    "print(reg_coef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
